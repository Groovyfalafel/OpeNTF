## Transformer Configuration

## Inspired by the Google setup
## From: https://opennmt.net/OpenNMT-py/FAQ.html#how-do-i-train-the-transformer-model

## For full options list for reference
## Visit: https://opennmt.net/OpenNMT-py/options/train.html



# [ Vocab ] ----------------------------

src_vocab: ../output/nmt/run/transformer.vocab.src
tgt_vocab: ../output/nmt/run/transformer.vocab.tgt
# share_vocab: False :default
decoder_start_token: 



## Where the samples will be written
save_data: ../output/nmt/run/transformer



# Prevent overwriting existing files in the folder
overwrite: True

# Corpus opts:
data:
  corpus_1:
    path_src: ../output/nmt/src-train.txt
    path_tgt: ../output/nmt/tgt-train.txt
    weight: 1
  valid:
    path_src: ../output/nmt/src-val.txt
    path_tgt: ../output/nmt/tgt-val.txt

# --------------------------------------------------------------------------

# General opts
save_model: ../output/nmt/run/transformer_model
save_checkpoint_steps: 200
warmup_steps: 200
valid_steps: 100
train_steps: 2504

# Batching
bucket_size: 10000
world_size: 1
gpu_ranks: [0]
num_workers: 8
batch_type: "tokens"
batch_size: 1024
valid_batch_size: 2048
accum_count: [4]

# Optimization
model_dtype: "fp16"
optim: adam
weight_decay: 0.0001
learning_rate: 0.01 # Set to a constant learning rate
adam_beta2: 0.998
max_grad_norm: 5
label_smoothing: 0.1
param_init: 0
param_init_glorot: true
normalization: "tokens"

# Model hyperparameters
encoder_type: transformer
decoder_type: transformer
position_encoding: true
max_relative_positions: 20
enc_layers: 8
dec_layers: 8
heads: 8
hidden_size: 512
word_vec_size: 512
transformer_ff: 2048
dropout: [0.2]
attention_dropout: [0.2]

beam_size: 12
length_penalty: 1.0
