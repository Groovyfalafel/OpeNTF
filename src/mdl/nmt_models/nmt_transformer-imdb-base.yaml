## Transformer Configuration

## Inspired by the Google setup
## from https://opennmt.net/OpenNMT-py/FAQ.html#how-do-i-train-the-transformer-model

## Where the samples will be written
save_data: ../output/nmt/run/transformer

## Where the vocab(s) will be written
src_vocab: ../output/nmt/run/transformer.vocab.src
tgt_vocab: ../output/nmt/run/transformer.vocab.tgt

# Prevent overwriting existing files in the folder
overwrite: True

# Corpus opts:
data:
  corpus_1:
    path_src: ../output/nmt/src-train.txt
    path_tgt: ../output/nmt/tgt-train.txt
    weight: 1
  valid:
    path_src: ../output/nmt/src-val.txt
    path_tgt: ../output/nmt/tgt-val.txt

# --------------------------------------------------------------------------

# General opts
save_model: ../output/nmt/run/transformer_model
train_steps: 100000 # default
save_checkpoint_steps: 10000 # default: 5000
valid_steps: 10000 # default
warmup_steps: 4000 # default
decay_steps: 10000 # default

# Batching
bucket_size: 4000
world_size: 2
gpu_ranks: [0, 1]
num_workers: 4
batch_type: "tokens"
batch_size: 448 # default of 64 * 7 tokens/seq avg
valid_batch_size: 224 # default of 32 * 7 tokens/seq avg
accum_count: [1] # No accumulation

# Optimization
model_dtype: "fp16" # 28.26 TFLOPS Tesla V100 (32GB)
optim: adam # base default
weight_decay: 0.0001
learning_rate: 0.0005 # default
decay_method: "noam"
adam_beta1: 0.9 # base default
adam_beta2: 0.98 # base default

label_smoothing: 0.1 # base default
param_init: 0.1 # default
param_init_glorot: true # required for transformer
normalization: "tokens"

# Model hyperparameters
encoder_type: transformer
decoder_type: transformer
position_encoding: false # base default
enc_layers: 6 # base default
dec_layers: 6 # base default
heads: 8 # base default
hidden_size: 512 # base default
word_vec_size: 512 # base default
transformer_ff: 2048 # base default
dropout: [0.1] # base default
attention_dropout: [0.1] # base default

self_attn_type: "scaled-dot" # default
beam_size: 4 # base default
length_penalty: 0.6 # base default
